# ğŸ  Deployment Local - Sin Modal

GuÃ­a para correr todo el sistema localmente sin usar Modal.

## ğŸ¯ Por quÃ© deployar local?

- âœ… **Gratis** - Sin costos de Modal
- âœ… **Privado** - Tus datos nunca salen de tu compu
- âœ… **RÃ¡pido** - Sin latencia de red (si tienes buena GPU)
- âœ… **Offline** - Funciona sin internet (despuÃ©s de descargar el modelo)
- âš ï¸ Requiere GPU/CPU potente para el LLM

## ğŸ“¦ Requisitos

- Python 3.11+
- ~4GB RAM mÃ­nimo (8GB recomendado)
- GPU NVIDIA (opcional pero recomendado)
- ~3GB espacio en disco para el modelo

## ğŸš€ Setup RÃ¡pido

### 1. Instalar dependencias

```bash
cd /Users/felipemaldonado/Documents/repositories/Orgappitation

# Dependencias bÃ¡sicas
pip install -r requirements.txt

# Para GPU NVIDIA (opcional, pero MUCHO mÃ¡s rÃ¡pido)
pip uninstall llama-cpp-python -y
CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python --no-cache-dir
```

### 2. Configurar .env

```bash
# Editar .env
nano .env

# Configurar para modo local:
FINANZAS_API_KEY=tu_api_key_secreta
TELEGRAM_BOT_TOKEN=tu_token_de_botfather

# URL local del LLM (importante!)
LLM_API_URL=http://127.0.0.1:8001/text_to_yaml

# Modal API URL (si quieres usar la API en Modal)
# O dÃ©jalo vacÃ­o si tambiÃ©n la quieres local
MODAL_API_URL=
```

## ğŸ® OpciÃ³n 1: Todo Local (LLM + API + Bot)

### 1. Iniciar Servicio LLM Local

Terminal 1:

```bash
python llm_service_local.py
```

Primera vez tarda ~5-10 minutos descargando el modelo (2GB).

DeberÃ­as ver:

```
ğŸ§  Cargando modelo Qwen-2.5-3b-Text_to_SQL...
   Repo: mradermacher/Qwen-2.5-3b-Text_to_SQL-GGUF
   File: Qwen-2.5-3b-Text_to_SQL.Q4_K_M.gguf
âœ… Modelo cargado exitosamente!

ğŸš€ Iniciando Finanzas LLM Service (Local)
   Host: 127.0.0.1
   Puerto: 8001
   URL: http://127.0.0.1:8001
```

### 2. Iniciar API Local (SQLite)

Terminal 2:

```bash
# Crear directorio para DB
mkdir -p data

# Inicializar DB local
python -c "
import sqlite3
with open('api/sql_schema.sql', 'r') as f:
    schema = f.read()
conn = sqlite3.connect('data/finanzas.db')
conn.executescript(schema)
conn.close()
print('âœ… DB local inicializada')
"

# Iniciar API local
python api/local_api.py
```

**Nota**: Si no existe `api/local_api.py`, puedes usar Modal localmente con:

```bash
modal serve api/modal_app.py
```

### 3. Iniciar Bot de Telegram

Terminal 3:

```bash
python telegram/bot.py
```

### 4. Probar

Abre Telegram y escribe:

```
GastÃ© 5000 en cafÃ©
```

## ğŸ® OpciÃ³n 2: HÃ­brido (LLM Local + API Modal)

Si quieres el LLM local pero la API en Modal (para acceso desde mÃºltiples dispositivos):

### 1. Deploy API en Modal

```bash
modal deploy api/modal_app.py
modal run api/modal_app.py::init_db
```

### 2. Configurar .env

```bash
# URL de Modal para la API
MODAL_API_URL=https://tu-url--finanzas-api-fastapi-app.modal.run

# URL local para el LLM
LLM_API_URL=http://127.0.0.1:8001/text_to_yaml
```

### 3. Iniciar servicios

```bash
# Terminal 1: LLM local
python llm_service_local.py

# Terminal 2: Bot
python telegram/bot.py
```

## ğŸ§ª Testing

### Test del LLM local

```bash
# Modo test directo (sin servidor)
python llm_service_local.py --test "GastÃ© 5000 en cafÃ©"

# Salida esperada:
ğŸ§ª Modo test: GastÃ© 5000 en cafÃ©

ğŸ§  Cargando modelo Qwen-2.5-3b-Text_to_SQL...
âœ… Modelo cargado exitosamente!

âœ… YAML generado:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
monto: 5000
descripcion: cafÃ©
categoria: food
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### Test del endpoint HTTP

```bash
# En otra terminal (con el servidor corriendo)
curl -X POST http://127.0.0.1:8001/text_to_yaml \
  -H "Content-Type: application/json" \
  -d '{
    "text": "GastÃ© 5000 en cafÃ©",
    "api_key": "tu_api_key"
  }'

# Respuesta esperada:
{
  "yaml_output": "monto: 5000\ndescripcion: cafÃ©\ncategoria: food",
  "success": true,
  "error": null
}
```

### Health check

```bash
curl http://127.0.0.1:8001/health

# Respuesta:
{
  "status": "ok",
  "service": "finanzas-llm-local",
  "model": "loaded"
}
```

## âš¡ Optimizaciones

### Si tienes GPU NVIDIA

```bash
# Reinstalar llama-cpp-python con soporte CUDA
pip uninstall llama-cpp-python -y
CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python --no-cache-dir

# Verificar GPU
python -c "from llama_cpp import Llama; print('GPU support:', Llama.supports_gpu_offload())"
```

Con GPU: ~2-3 segundos por request
Sin GPU: ~10-20 segundos por request

### Si solo tienes CPU

El modelo igual funciona, solo serÃ¡ mÃ¡s lento.

Para optimizar:

```bash
# Usar modelo mÃ¡s pequeÃ±o
# Edita llm_service_local.py, lÃ­nea ~23:
MODEL_FILE = "Qwen-2.5-3b-Text_to_SQL.Q2_K.gguf"  # MÃ¡s pequeÃ±o, menos preciso
```

### Cambiar puerto

```bash
# Si el puerto 8001 estÃ¡ ocupado
python llm_service_local.py --port 8002

# Actualizar .env:
LLM_API_URL=http://127.0.0.1:8002/text_to_yaml
```

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Ajustar threads (CPU)

Edita `llm_service_local.py`, lÃ­nea ~120:

```python
n_threads=os.cpu_count() or 8,  # Ajusta segÃºn tus cores
```

### Ajustar context size

Si tienes suficiente RAM, puedes aumentar el contexto:

```python
n_ctx=8192,  # Default es 4096
```

### Deshabilitar GPU

Si tienes GPU pero quieres usar CPU:

```python
n_gpu_layers=0,  # 0 = solo CPU, -1 = toda la GPU
```

## ğŸ“Š ComparaciÃ³n: Local vs Modal

| CaracterÃ­stica | Local | Modal |
|---------------|-------|-------|
| **Costo** | $0 | ~$3/mes |
| **Velocidad (GPU)** | 2-3s | 3-5s (30-60s cold start) |
| **Velocidad (CPU)** | 10-20s | N/A (Modal usa GPU) |
| **Setup** | 10 min | 5 min |
| **Privacidad** | 100% local | En la nube |
| **Acceso remoto** | Solo local | Desde cualquier lugar |
| **Mantenimiento** | Tu responsabilidad | AutomÃ¡tico |

## ğŸ› Troubleshooting

### "ImportError: llama_cpp"

```bash
pip install llama-cpp-python
```

### "Model download failed"

El modelo se descarga desde HuggingFace automÃ¡ticamente. Si falla:

```bash
# Descargar manualmente
huggingface-cli download mradermacher/Qwen-2.5-3b-Text_to_SQL-GGUF \
  Qwen-2.5-3b-Text_to_SQL.Q4_K_M.gguf

# O usar el script
python -c "
from llama_cpp import Llama
Llama.from_pretrained(
    repo_id='mradermacher/Qwen-2.5-3b-Text_to_SQL-GGUF',
    filename='Qwen-2.5-3b-Text_to_SQL.Q4_K_M.gguf',
)
print('âœ… Modelo descargado')
"
```

### "Address already in use"

Puerto 8001 ocupado. Usa otro:

```bash
python llm_service_local.py --port 8002
```

### Muy lento (CPU)

Si solo tienes CPU y es muy lento:

1. Usa modelo mÃ¡s pequeÃ±o (Q2_K en vez de Q4_K_M)
2. Reduce threads: `n_threads=4`
3. O usa Modal (tiene GPU)

### "CUDA out of memory"

Tu GPU tiene poca memoria. Reduce layers:

```python
n_gpu_layers=20,  # En vez de -1 (todo)
```

## ğŸ”„ Mantener actualizado

```bash
# Actualizar dependencias
pip install -r requirements.txt --upgrade

# Actualizar modelo (si hay nueva versiÃ³n)
rm -rf ~/.cache/huggingface/hub/models--mradermacher--*
python llm_service_local.py --test "test"  # Re-descarga
```

## ğŸš¦ Iniciar automÃ¡ticamente

### macOS/Linux - systemd

Crea `/etc/systemd/system/finanzas-llm.service`:

```ini
[Unit]
Description=Finanzas LLM Service
After=network.target

[Service]
Type=simple
User=tu_usuario
WorkingDirectory=/Users/felipemaldonado/Documents/repositories/Orgappitation
ExecStart=/usr/bin/python3 llm_service_local.py
Restart=always

[Install]
WantedBy=multi-user.target
```

```bash
sudo systemctl enable finanzas-llm
sudo systemctl start finanzas-llm
```

### macOS - LaunchAgent

Crea `~/Library/LaunchAgents/com.finanzas.llm.plist`:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
    <key>Label</key>
    <string>com.finanzas.llm</string>
    <key>ProgramArguments</key>
    <array>
        <string>/usr/local/bin/python3</string>
        <string>/Users/felipemaldonado/Documents/repositories/Orgappitation/llm_service_local.py</string>
    </array>
    <key>RunAtLoad</key>
    <true/>
    <key>KeepAlive</key>
    <true/>
</dict>
</plist>
```

```bash
launchctl load ~/Library/LaunchAgents/com.finanzas.llm.plist
```

## ğŸ“ Resumen

**Para desarrollo/testing**: Local es perfecto
**Para producciÃ³n 24/7**: Modal es mÃ¡s confiable
**Para privacidad mÃ¡xima**: Local + SQLite local
**Para acceso remoto**: HÃ­brido (LLM local + API Modal)

---

Ver tambiÃ©n:
- [DEPLOY_INSTRUCTIONS.md](DEPLOY_INSTRUCTIONS.md) - Deploy en Modal
- [ARCHITECTURE.md](ARCHITECTURE.md) - Arquitectura del sistema
- [README.md](README.md) - Overview general
